# Resume Diagnosis Engine - Environment Variables
# Copy this file to .env and fill in your values

# ======================
# Database Configuration
# ======================
# Neon PostgreSQL connection string (pooled for production)
DATABASE_URL=postgresql://user:password@host/database?sslmode=require
# Unpooled connection for migrations
DATABASE_URL_UNPOOLED=postgresql://user:password@host/database?sslmode=require

# ======================
# Redis Configuration
# ======================
# Redis connection URL
REDIS_URL=redis://default:password@host:port

# ======================
# Application Settings
# ======================
NODE_ENV=development
PORT=3000

# ======================
# Security Settings
# ======================
# JWT secret for session tokens (min 32 characters)
JWT_SECRET=your-super-secret-jwt-key-min-32-characters

# Encryption key for resume data (exactly 32 characters)
ENCRYPTION_KEY=your-32-character-encryption-key

# Admin API key for migrations and admin endpoints
ADMIN_KEY=your-admin-api-key

# ======================
# AI Configuration
# ======================
# Groq API key (get from https://console.groq.com)
GROQ_API_KEY=your-groq-api-key

# Model to use for analysis
GROQ_MODEL=llama3-8b-8192

# ======================
# CORS Settings
# ======================
# Allowed origins (use * for development, specific domain for production)
CORS_ORIGIN=http://localhost:3000

# ======================
# File Upload Limits
# ======================
# Maximum file size in bytes (10MB = 10485760)
MAX_FILE_SIZE=10485760

# Maximum resume pages (per Requirement 10)
MAX_RESUME_PAGES=10

# Upload directory for temp files
UPLOAD_DIR=/tmp/uploads

# ======================
# Processing Timeouts
# ======================
# Upload processing timeout in seconds (Requirement 1)
UPLOAD_TIMEOUT=30

# AI analysis timeout in seconds (Requirement 3)
ANALYSIS_TIMEOUT=60

# Maximum total processing time (Requirement 10)
MAX_PROCESSING_TIMEOUT=120

# ======================
# Data Retention
# ======================
# Hours before data is deleted (Requirement 9)
DATA_TTL_HOURS=24

# Cleanup job interval in minutes
CLEANUP_INTERVAL_MINUTES=60
